{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_size = 28\n",
    "channels = 1\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "lambda_gp = 10\n",
    "n_critic = 5  # Train critic more frequently\n",
    "epochs = 150  \n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "data = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Linear(in_features=512, out_features=784, bias=True)\n",
       "    (9): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, channels * img_size * img_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), channels, img_size, img_size)\n",
    "        return img\n",
    "\n",
    "generator = Generator().to(device)\n",
    "generator.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(channels * img_size * img_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Penalty\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(device)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150] | D Loss: -2.0150 | G Loss: -7.6286\n",
      "Epoch [2/150] | D Loss: -1.7162 | G Loss: -9.6233\n",
      "Epoch [3/150] | D Loss: -1.6967 | G Loss: -9.2962\n",
      "Epoch [4/150] | D Loss: -1.2508 | G Loss: -6.4426\n",
      "Epoch [5/150] | D Loss: -2.0948 | G Loss: -3.3744\n",
      "Epoch [6/150] | D Loss: -2.6375 | G Loss: -2.9998\n",
      "Epoch [7/150] | D Loss: -2.9771 | G Loss: -1.6837\n",
      "Epoch [8/150] | D Loss: -2.7262 | G Loss: -5.0573\n",
      "Epoch [9/150] | D Loss: -2.4172 | G Loss: -3.0480\n",
      "Epoch [10/150] | D Loss: -2.1630 | G Loss: -3.5768\n",
      "Epoch [11/150] | D Loss: -2.3166 | G Loss: -4.2820\n",
      "Epoch [12/150] | D Loss: -1.7793 | G Loss: -5.3115\n",
      "Epoch [13/150] | D Loss: -1.7578 | G Loss: -4.3544\n",
      "Epoch [14/150] | D Loss: -1.4586 | G Loss: -3.8592\n",
      "Epoch [15/150] | D Loss: -1.6961 | G Loss: -4.1401\n",
      "Epoch [16/150] | D Loss: -1.9963 | G Loss: -3.6713\n",
      "Epoch [17/150] | D Loss: -1.6805 | G Loss: -4.8006\n",
      "Epoch [18/150] | D Loss: -1.5447 | G Loss: -3.9769\n",
      "Epoch [19/150] | D Loss: -1.1086 | G Loss: -4.9399\n",
      "Epoch [20/150] | D Loss: -1.6587 | G Loss: -4.9157\n",
      "Epoch [21/150] | D Loss: -1.1248 | G Loss: -4.8657\n",
      "Epoch [22/150] | D Loss: -0.9384 | G Loss: -5.5027\n",
      "Epoch [23/150] | D Loss: -0.9932 | G Loss: -3.8033\n",
      "Epoch [24/150] | D Loss: -1.2681 | G Loss: -2.9557\n",
      "Epoch [25/150] | D Loss: -1.5338 | G Loss: -2.7105\n",
      "Epoch [26/150] | D Loss: -0.5626 | G Loss: -2.3943\n",
      "Epoch [27/150] | D Loss: -1.0269 | G Loss: -3.6450\n",
      "Epoch [28/150] | D Loss: -1.1331 | G Loss: -3.8718\n",
      "Epoch [29/150] | D Loss: -1.3021 | G Loss: -2.5672\n",
      "Epoch [30/150] | D Loss: -1.6937 | G Loss: -2.5805\n",
      "Epoch [31/150] | D Loss: -1.4711 | G Loss: -3.8957\n",
      "Epoch [32/150] | D Loss: -0.7677 | G Loss: -1.3793\n",
      "Epoch [33/150] | D Loss: -1.4394 | G Loss: -2.1012\n",
      "Epoch [34/150] | D Loss: -0.6280 | G Loss: -3.5075\n",
      "Epoch [35/150] | D Loss: -0.6773 | G Loss: -4.1358\n",
      "Epoch [36/150] | D Loss: -0.2426 | G Loss: -2.8751\n",
      "Epoch [37/150] | D Loss: -0.9873 | G Loss: -2.0828\n",
      "Epoch [38/150] | D Loss: -1.0891 | G Loss: -2.9451\n",
      "Epoch [39/150] | D Loss: -0.9790 | G Loss: -2.2114\n",
      "Epoch [40/150] | D Loss: -0.7852 | G Loss: -2.7116\n",
      "Epoch [41/150] | D Loss: -0.6859 | G Loss: -2.8526\n",
      "Epoch [42/150] | D Loss: -1.4237 | G Loss: -2.0975\n",
      "Epoch [43/150] | D Loss: -1.2231 | G Loss: -3.7514\n",
      "Epoch [44/150] | D Loss: -1.3341 | G Loss: -3.2488\n",
      "Epoch [45/150] | D Loss: -1.1564 | G Loss: -2.6063\n",
      "Epoch [46/150] | D Loss: -1.3502 | G Loss: -2.7421\n",
      "Epoch [47/150] | D Loss: -1.2246 | G Loss: -1.0954\n",
      "Epoch [48/150] | D Loss: -0.9506 | G Loss: -1.2286\n",
      "Epoch [49/150] | D Loss: -0.9609 | G Loss: -1.3030\n",
      "Epoch [50/150] | D Loss: -1.2055 | G Loss: -2.8804\n",
      "Epoch [51/150] | D Loss: -1.4353 | G Loss: -2.5117\n",
      "Epoch [52/150] | D Loss: -1.1818 | G Loss: -1.5909\n",
      "Epoch [53/150] | D Loss: -0.6055 | G Loss: -1.2947\n",
      "Epoch [54/150] | D Loss: -1.2272 | G Loss: -1.8118\n",
      "Epoch [55/150] | D Loss: -1.1474 | G Loss: -1.9242\n",
      "Epoch [56/150] | D Loss: -1.3205 | G Loss: -0.8705\n",
      "Epoch [57/150] | D Loss: -1.0809 | G Loss: -1.8185\n",
      "Epoch [58/150] | D Loss: -0.9114 | G Loss: -2.8604\n",
      "Epoch [59/150] | D Loss: -1.1788 | G Loss: -0.7898\n",
      "Epoch [60/150] | D Loss: -1.3283 | G Loss: -1.1842\n",
      "Epoch [61/150] | D Loss: -0.8310 | G Loss: -1.1667\n",
      "Epoch [62/150] | D Loss: -1.1377 | G Loss: -1.8143\n",
      "Epoch [63/150] | D Loss: -0.5280 | G Loss: -1.9352\n",
      "Epoch [64/150] | D Loss: -1.1665 | G Loss: -1.2651\n",
      "Epoch [65/150] | D Loss: -0.5742 | G Loss: -1.4631\n",
      "Epoch [66/150] | D Loss: -0.6418 | G Loss: -2.1497\n",
      "Epoch [67/150] | D Loss: -1.1885 | G Loss: -2.8193\n",
      "Epoch [68/150] | D Loss: -0.4698 | G Loss: -0.5427\n",
      "Epoch [69/150] | D Loss: -0.4253 | G Loss: 0.3520\n",
      "Epoch [70/150] | D Loss: -1.0382 | G Loss: -3.0107\n",
      "Epoch [71/150] | D Loss: -0.9857 | G Loss: -1.0019\n",
      "Epoch [72/150] | D Loss: -1.1440 | G Loss: -1.0916\n",
      "Epoch [73/150] | D Loss: -1.1946 | G Loss: -1.6379\n",
      "Epoch [74/150] | D Loss: -0.8638 | G Loss: -0.5882\n",
      "Epoch [75/150] | D Loss: -1.1413 | G Loss: -1.4619\n",
      "Epoch [76/150] | D Loss: -1.3821 | G Loss: -0.7364\n",
      "Epoch [77/150] | D Loss: -0.6301 | G Loss: -0.8577\n",
      "Epoch [78/150] | D Loss: -0.8203 | G Loss: 0.2198\n",
      "Epoch [79/150] | D Loss: -1.0487 | G Loss: 0.3983\n",
      "Epoch [80/150] | D Loss: -0.9486 | G Loss: -2.0596\n",
      "Epoch [81/150] | D Loss: -1.0347 | G Loss: -0.9466\n",
      "Epoch [82/150] | D Loss: -0.5369 | G Loss: -1.1654\n",
      "Epoch [83/150] | D Loss: -1.3484 | G Loss: -0.5813\n",
      "Epoch [84/150] | D Loss: -0.5012 | G Loss: -1.0854\n",
      "Epoch [85/150] | D Loss: -1.4502 | G Loss: -1.1306\n",
      "Epoch [86/150] | D Loss: -0.8227 | G Loss: -1.3299\n",
      "Epoch [87/150] | D Loss: -0.8963 | G Loss: -0.9355\n",
      "Epoch [88/150] | D Loss: -0.7606 | G Loss: -1.6539\n",
      "Epoch [89/150] | D Loss: -0.6888 | G Loss: -0.3964\n",
      "Epoch [90/150] | D Loss: -0.9287 | G Loss: 0.3664\n",
      "Epoch [91/150] | D Loss: -0.5168 | G Loss: -1.0042\n",
      "Epoch [92/150] | D Loss: -1.2662 | G Loss: -0.8233\n",
      "Epoch [93/150] | D Loss: -0.6328 | G Loss: -0.8424\n",
      "Epoch [94/150] | D Loss: -1.0094 | G Loss: -1.9516\n",
      "Epoch [95/150] | D Loss: -1.1963 | G Loss: -0.3292\n",
      "Epoch [96/150] | D Loss: -1.3995 | G Loss: -0.8061\n",
      "Epoch [97/150] | D Loss: -0.6610 | G Loss: 0.3192\n",
      "Epoch [98/150] | D Loss: -0.3728 | G Loss: -1.4726\n",
      "Epoch [99/150] | D Loss: -0.4677 | G Loss: -1.4590\n",
      "Epoch [100/150] | D Loss: -0.6607 | G Loss: -2.2000\n",
      "Epoch [101/150] | D Loss: -1.3888 | G Loss: -1.2242\n",
      "Epoch [102/150] | D Loss: -1.0722 | G Loss: -0.6191\n",
      "Epoch [103/150] | D Loss: -0.6570 | G Loss: -0.6963\n",
      "Epoch [104/150] | D Loss: -0.9106 | G Loss: -1.7181\n",
      "Epoch [105/150] | D Loss: -0.5144 | G Loss: -1.9264\n",
      "Epoch [106/150] | D Loss: -0.7204 | G Loss: -1.0800\n",
      "Epoch [107/150] | D Loss: -1.0866 | G Loss: -0.6589\n",
      "Epoch [108/150] | D Loss: -0.4244 | G Loss: -1.4495\n",
      "Epoch [109/150] | D Loss: -0.7716 | G Loss: -0.8681\n",
      "Epoch [110/150] | D Loss: -0.7391 | G Loss: -0.2215\n",
      "Epoch [111/150] | D Loss: -0.8998 | G Loss: -0.5789\n",
      "Epoch [112/150] | D Loss: -1.1009 | G Loss: -0.7854\n",
      "Epoch [113/150] | D Loss: -0.7899 | G Loss: -0.7423\n",
      "Epoch [114/150] | D Loss: -1.0938 | G Loss: -1.0178\n",
      "Epoch [115/150] | D Loss: -0.6432 | G Loss: -1.4027\n",
      "Epoch [116/150] | D Loss: -0.5920 | G Loss: -0.4470\n",
      "Epoch [117/150] | D Loss: -1.3612 | G Loss: -0.2319\n",
      "Epoch [118/150] | D Loss: -0.6311 | G Loss: -2.2729\n",
      "Epoch [119/150] | D Loss: -0.7816 | G Loss: -0.4039\n",
      "Epoch [120/150] | D Loss: -0.4103 | G Loss: -0.3043\n",
      "Epoch [121/150] | D Loss: -0.7821 | G Loss: -1.1448\n",
      "Epoch [122/150] | D Loss: -0.8297 | G Loss: -0.1355\n",
      "Epoch [123/150] | D Loss: -0.9750 | G Loss: -0.7041\n",
      "Epoch [124/150] | D Loss: -0.2924 | G Loss: -1.5976\n",
      "Epoch [125/150] | D Loss: -0.7320 | G Loss: -1.1599\n",
      "Epoch [126/150] | D Loss: -1.3168 | G Loss: -1.4288\n",
      "Epoch [127/150] | D Loss: -1.1008 | G Loss: -0.2345\n",
      "Epoch [128/150] | D Loss: -1.1428 | G Loss: -1.2441\n",
      "Epoch [129/150] | D Loss: -0.8757 | G Loss: -0.7066\n",
      "Epoch [130/150] | D Loss: -0.7164 | G Loss: -1.3597\n",
      "Epoch [131/150] | D Loss: -0.6598 | G Loss: -1.6761\n",
      "Epoch [132/150] | D Loss: -0.6424 | G Loss: -0.6445\n",
      "Epoch [133/150] | D Loss: -0.9506 | G Loss: 0.4905\n",
      "Epoch [134/150] | D Loss: -0.9674 | G Loss: -0.8730\n",
      "Epoch [135/150] | D Loss: -0.8888 | G Loss: -2.0248\n",
      "Epoch [136/150] | D Loss: -0.6036 | G Loss: -1.6415\n",
      "Epoch [137/150] | D Loss: -0.6840 | G Loss: 1.2918\n",
      "Epoch [138/150] | D Loss: -0.6049 | G Loss: -1.2679\n",
      "Epoch [139/150] | D Loss: -1.0623 | G Loss: -0.0357\n",
      "Epoch [140/150] | D Loss: -0.9164 | G Loss: -2.1537\n",
      "Epoch [141/150] | D Loss: -0.6765 | G Loss: -1.5119\n",
      "Epoch [142/150] | D Loss: -1.1372 | G Loss: -0.7729\n",
      "Epoch [143/150] | D Loss: -0.2578 | G Loss: -1.1120\n",
      "Epoch [144/150] | D Loss: -0.7263 | G Loss: -1.5745\n",
      "Epoch [145/150] | D Loss: -0.5394 | G Loss: -1.9583\n",
      "Epoch [146/150] | D Loss: -0.9752 | G Loss: -1.4884\n",
      "Epoch [147/150] | D Loss: -1.4147 | G Loss: -1.1921\n",
      "Epoch [148/150] | D Loss: -0.9338 | G Loss: -1.7879\n",
      "Epoch [149/150] | D Loss: -0.8616 | G Loss: -0.6133\n",
      "Epoch [150/150] | D Loss: -1.2852 | G Loss: 0.0332\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # Train Critic\n",
    "        optimizer_D.zero_grad()\n",
    "        z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "        fake_imgs = generator(z)\n",
    "        real_validity = discriminator(imgs)\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, imgs, fake_imgs)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_imgs = generator(z)\n",
    "            g_loss = -torch.mean(discriminator(fake_imgs))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# Save models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(generator.state_dict(), \"models/generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"models/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random images \n",
    "z = torch.randn(5000, latent_dim).to(device)\n",
    "generated_imgs = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Generated Images\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "for i, img in enumerate(generated_imgs[:25]):\n",
    "    img = img.cpu().detach().numpy().squeeze()\n",
    "    img = ((img + 1) / 2) * 255  # Rescale to [0, 255]\n",
    "    img = Image.fromarray(img.astype(np.uint8))\n",
    "    img.save(f\"generated_images/img_{i + 1}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
